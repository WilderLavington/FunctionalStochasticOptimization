{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "6817e1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import wandb\n",
    "api = wandb.Api(timeout=180)\n",
    "import os\n",
    "import pandas as pd\n",
    "import wandb\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "from copy import deepcopy\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import argparse\n",
    "import itertools\n",
    "import  matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import itertools\n",
    "import time\n",
    "import matplotlib as mpl\n",
    "import matplotlib.ticker as ticker\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "03fa6653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists\n"
     ]
    }
   ],
   "source": [
    "USER='wilderlavington' \n",
    "PROJECT='FunctionalOptimizationFinal'\n",
    "SUMMARY_FILE='FuncOptSVMlib.csv'  \n",
    "try:\n",
    "    os.makedirs(\"plots/aistats/\")\n",
    "except FileExistsError:\n",
    "    print(\"File already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "0579afb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run plotting_utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6745cfe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|███████████████████████████████████▍   | 1342/1475 [04:05<00:21,  6.27it/s]"
     ]
    }
   ],
   "source": [
    "download_wandb_summary(user=USER, project=PROJECT, summary_file=SUMMARY_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572f4f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "download_wandb_records(user=USER, project=PROJECT, summary_file=SUMMARY_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd9d1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sgd_figure(loss, schedule, wandb_records, fig_name, x ='optim_steps', y='avg_loss', include_leg=True):\n",
    "    \n",
    "    # base info   \n",
    "    dataset_names = ['ijcnn', 'rcv1'] #'mushrooms', \n",
    "    batch_sizes = [25, 125, 625]\n",
    "    m = [1, 5, 10, 100] \n",
    "    wandb_records = wandb_records[wandb_records['group']=='AIstats_narval']\n",
    "    # init plots \n",
    "    fig, axs = plt.subplots(len(dataset_names), len(batch_sizes)+1, figsize=(16, 3))\n",
    "    colors = mpl.cm.Set1.colors   # Qualitative colormap\n",
    "    colormap = {'SGD': '#44AA99' , 'SLS': '#DDCC77', 'Adam': '#88CCEE'}\n",
    "    colormap.update({'SSO-1':  '#CC6677' ,  'SSO-5': '#AA4499', 'SSO-10': '#882255' , 'SSO-100': '#332288'})\n",
    "    algorithms = ['SGD', 'Adam', 'SLS'] + ['SSO-'+str(m_) for m_ in m] \n",
    "    label_map = {x:'Optimization-Steps', y:'Gradient-Norm'}\n",
    "    \n",
    "    # now add in the lines to each of the plots \n",
    "    for row, dataset_name in enumerate(dataset_names):\n",
    "        \n",
    "        # figure out axis automatically \n",
    "        x_max = 0 \n",
    "\n",
    "        # SLS\n",
    "        proc_df = format_dataframe(wandb_records,\n",
    "            id_subfields={'fullbatch': 1,  \n",
    "            'use_optimal_stepsize': 1, 'loss': loss, 'algo': 'LSOpt',\n",
    "            'eta_schedule': schedule, 'dataset_name': dataset_name},\n",
    "            x_col=x , y_col=y) \n",
    "        if proc_df is not None:\n",
    "            x_max = max(proc_df[x].values.max(), x_max)\n",
    "            axs[row][-1] = generate_plot(proc_df, x, y, axs[row][-1], label='SLS', \n",
    "                                         linestyle='dashed', color=colormap['SLS'])\n",
    "        else:\n",
    "            print('missing SLS  ', dataset_name, 'full-batch')\n",
    "\n",
    "        # SGD\n",
    "        proc_df = format_dataframe(wandb_records,\n",
    "            id_subfields={'fullbatch': 1,  \n",
    "            'use_optimal_stepsize': 1, 'loss': loss, 'algo': 'SGD',\n",
    "            'eta_schedule': schedule, 'dataset_name': dataset_name},\n",
    "            x_col=x , y_col=y) \n",
    "        if proc_df is not None: \n",
    "            x_max = max(proc_df[x].values.max(), x_max)\n",
    "            axs[row][-1] = generate_plot(proc_df, x, y, axs[row][-1], label='SGD', \n",
    "                                         linestyle='dashed', color=colormap['SGD'])\n",
    "        else:\n",
    "            print('missing SGD  ', dataset_name, 'full-batch')\n",
    "        \n",
    "        # Adam\n",
    "        proc_df = format_dataframe(wandb_records,\n",
    "            id_subfields={'fullbatch': 1,  \n",
    "            'use_optimal_stepsize': 1, 'loss': loss, 'algo': 'Adam',\n",
    "            'eta_schedule': 'constant', 'dataset_name': dataset_name},\n",
    "            x_col=x , y_col=y) \n",
    "        if proc_df is not None: \n",
    "            x_max = max(proc_df[x].values.max(), x_max)\n",
    "            axs[row][-1] = generate_plot(proc_df, x, y, axs[row][-1], label='Adam', \n",
    "                                         linestyle='dashed', color=colormap['Adam'])\n",
    "        else:\n",
    "            print('missing Adam  ', dataset_name, 'full-batch')\n",
    "        \n",
    "        # FMDopt theoretical \n",
    "        for m_ in m:\n",
    "            # create parsed info \n",
    "            proc_df = format_dataframe(wandb_records, \n",
    "                id_subfields={'fullbatch': 1,   \n",
    "                    'use_optimal_stepsize': 1, \n",
    "                    'loss': loss, 'algo': 'SGD_FMDOpt', 'm': m_,\n",
    "                    'eta_schedule': schedule, 'dataset_name': dataset_name}, \n",
    "                     avg_subfields=['seed'], max_subfields=['c'],\n",
    "                x_col=x, y_col=y)\n",
    "            if proc_df is not None:\n",
    "                x_max = max(proc_df[x].values.max(), x_max)\n",
    "                axs[row][-1] = generate_plot(proc_df, x, y, axs[row][-1],  \\\n",
    "                                        label='SSO-'+str(m_), linestyle='solid', color=colormap['SSO-'+str(m_)])\n",
    "            else:\n",
    "                print('missing FMDopt  ', m_, dataset_name, 'full-batch') \n",
    "        axs[row][-1].grid()     \n",
    "        axs[row][-1].set_yscale(\"log\")\n",
    "        axs[row][-1].set_xscale(\"log\") \n",
    "        \n",
    "        # mini-batch plots \n",
    "        for col, batch_size in enumerate(batch_sizes):\n",
    "            \n",
    "            # figure out axis automatically \n",
    "            x_max = 0 \n",
    "            \n",
    "            # SLS\n",
    "            proc_df = format_dataframe(wandb_records,\n",
    "                id_subfields={'batch_size': batch_size,  #'_step': 499.0,\n",
    "                'use_optimal_stepsize': 1, 'loss': loss, 'algo': 'LSOpt',\n",
    "                'eta_schedule': schedule, 'dataset_name': dataset_name},\n",
    "                x_col=x , y_col=y) \n",
    "            if proc_df is not None:\n",
    "                x_max = max(proc_df[x].values.max(), x_max)\n",
    "                axs[row][col] = generate_plot(proc_df, x, y, axs[row][col], label='SLS', \n",
    "                                             linestyle='dashed', color=colormap['SLS'])\n",
    "            else:\n",
    "                print('missing SLS  ', dataset_name, batch_size)\n",
    "            \n",
    "            # SGD\n",
    "            proc_df = format_dataframe(wandb_records,\n",
    "                id_subfields={'batch_size': batch_size, #'_step': 499.0,\n",
    "                'use_optimal_stepsize': 1, 'loss': loss, 'algo': 'SGD',\n",
    "                'eta_schedule': schedule, 'dataset_name': dataset_name},\n",
    "                x_col=x , y_col=y) \n",
    "            if proc_df is not None: \n",
    "                x_max = max(proc_df[x].values.max(), x_max)\n",
    "                axs[row][col] = generate_plot(proc_df, x, y, axs[row][col], label='SGD', \n",
    "                                             linestyle='dashed', color=colormap['SGD'])\n",
    "            else:\n",
    "                print('missing SGD  ', dataset_name, batch_size)\n",
    "            \n",
    "            # Adam\n",
    "            proc_df = format_dataframe(wandb_records,\n",
    "                id_subfields={'batch_size': batch_size, #'_step': 499.0,  \n",
    "                'use_optimal_stepsize': 1, 'loss': loss, 'algo': 'Adam',\n",
    "                'eta_schedule': 'constant', 'dataset_name': dataset_name},\n",
    "                x_col=x , y_col=y) \n",
    "            if proc_df is not None: \n",
    "                x_max = max(proc_df[x].values.max(), x_max)\n",
    "                axs[row][col] = generate_plot(proc_df, x, y, axs[row][col], label='Adam', \n",
    "                                             linestyle='dashed', color=colormap['Adam'])\n",
    "            else:\n",
    "                print('missing Adam  ', dataset_name, batch_size)\n",
    "    \n",
    "            # FMDopt theoretical \n",
    "            for m_ in m:\n",
    "                # create parsed info \n",
    "                proc_df = format_dataframe(wandb_records, \n",
    "                    id_subfields={'batch_size': batch_size,  \n",
    "                        'use_optimal_stepsize': 1, #'_step': 499.0,\n",
    "                        'loss': loss, 'algo': 'SGD_FMDOpt', 'm': m_,\n",
    "                        'eta_schedule': schedule, 'dataset_name': dataset_name}, \n",
    "                         avg_subfields=['seed'], max_subfields=['c'],\n",
    "                    x_col=x, y_col=y)\n",
    "                if proc_df is not None:\n",
    "                    x_max = max(proc_df[x].values.max(), x_max)\n",
    "                    axs[row][col] = generate_plot(proc_df, x, y, axs[row][col], \\\n",
    "                                            label='SSO-'+str(m_), linestyle='solid', color=colormap['SSO-'+str(m_)])\n",
    "                else:\n",
    "                    print('missing FMDopt  ', m_, dataset_name, batch_size) \n",
    "            \n",
    "            axs[row][col].grid()     \n",
    "            axs[row][col].ticklabel_format(axis='x', style='sci', scilimits=(0,0))\n",
    "            axs[row][col].yaxis.set_major_locator(plt.MaxNLocator(4))\n",
    "            axs[row][col].set_yscale(\"log\")\n",
    "            axs[row][col].set_xscale(\"log\")\n",
    "            if not include_leg:\n",
    "                axs[0][col].set_title('batch-size: '+str(batch_size), fontsize=22)\n",
    "                axs[0][-1].set_title('full-batch', fontsize=22)\n",
    "            axs[row][-1].set_ylabel(dataset_name, fontsize=22)\n",
    "            axs[row][-1].yaxis.set_label_position(\"right\") \n",
    "        \n",
    "        axs[row][col].xaxis.set_minor_locator(mpl.ticker.LogLocator(base=10,numticks=100))\n",
    "        axs[row][col].xaxis.set_minor_formatter(mpl.ticker.NullFormatter()) \n",
    "        axs[row][col].yaxis.set_minor_formatter(mpl.ticker.NullFormatter())\n",
    "        \n",
    "    # remaining format stuff  \n",
    "    if include_leg:\n",
    "        handles = [mpatches.Patch(color=colormap[algo], label=algo) for algo in algorithms]\n",
    "        leg = fig.legend(handles=handles,\n",
    "               loc=\"lower center\",   # Position of legend\n",
    "               borderaxespad=1.65,    # Small spacing around legend box\n",
    "               # title=\"Algorithms\",  # Title for the legend\n",
    "               fontsize=18,\n",
    "               ncol=7, \n",
    "               bbox_to_anchor=(0.525, -0.3),\n",
    "               )\n",
    "    \n",
    "    plt.subplots_adjust(hspace=1.5)\n",
    "    plt.rcParams['figure.dpi'] = 100# 400 \n",
    "    fig.tight_layout()\n",
    "    \n",
    "    # show / save\n",
    "    plt.savefig('plots/aistats/workshop-plot-sgd_sso-'+fig_name+loss+'.pdf', bbox_inches='tight')\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ea348d",
   "metadata": {},
   "outputs": [],
   "source": [
    " wandb_records = pd.read_csv('logs/wandb_data/__full__'+SUMMARY_FILE, header=0, squeeze=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b7836a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss comparison\n",
    "generate_sgd_figure('MSELoss', 'constant', wandb_records, fig_name=SUMMARY_FILE+'_a', x ='optim_steps', y='avg_loss', include_leg=False)\n",
    "\n",
    "# grad-norm comparison\n",
    "# generate_sgd_figure('MSELoss', 'constant', wandb_records, fig_name='a', x ='optim_steps', y='grad_norm')\n",
    "# generate_sgd_figure('BCEWithLogitsLoss', 'constant', wandb_records, fig_name='a', x ='optim_steps', y='grad_norm')\n",
    "# # time-elapsed comparison\n",
    "# generate_sgd_figure('MSELoss', 'constant', wandb_records, fig_name='a', x ='time_elapsed', y='avg_loss')\n",
    "# generate_sgd_figure('BCEWithLogitsLoss', 'constant', wandb_records, fig_name='a', x ='time_elapsed', y='avg_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495b18a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_sgd_figure('BCEWithLogitsLoss', 'constant', wandb_records, fig_name=SUMMARY_FILE+'_a', x ='optim_steps', y='avg_loss', include_leg=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588a8b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_sgd_figure('BCEWithLogitsLoss', 'constant', wandb_records, fig_name=SUMMARY_FILE+'_aleg', x ='optim_steps', y='avg_loss', include_leg=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9f1a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss comparison\n",
    "# generate_sgd_figure('MSELoss', 'stochastic', wandb_records, fig_name=SUMMARY_FILE+'b', x ='optim_steps', y='avg_loss')\n",
    "# generate_sgd_figure('BCEWithLogitsLoss', 'stochastic', wandb_records, fig_name=SUMMARY_FILE+'b', x ='optim_steps', y='avg_loss')\n",
    "# grad-norm comparison\n",
    "# generate_sgd_figure('MSELoss', 'stochastic', wandb_records, fig_name='b', x ='optim_steps', y='grad_norm')\n",
    "# generate_sgd_figure('BCEWithLogitsLoss', 'stochastic', wandb_records, fig_name='b', x ='optim_steps', y='grad_norm')\n",
    "# time-elapsed comparison\n",
    "# generate_sgd_figure('MSELoss', 'stochastic', wandb_records, fig_name='b', x ='time_elapsed', y='avg_loss')\n",
    "# generate_sgd_figure('BCEWithLogitsLoss', 'stochastic', wandb_records, fig_name='b', x ='time_elapsed', y='avg_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be4c569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss comparison\n",
    "# generate_sgd_figure('MSELoss', 'exponential', wandb_records, fig_name=SUMMARY_FILE+'c', x ='optim_steps', y='avg_loss')\n",
    "# generate_sgd_figure('BCEWithLogitsLoss', 'exponential', wandb_records, fig_name=SUMMARY_FILE+'c', x ='optim_steps', y='avg_loss')\n",
    "# grad-norm comparison\n",
    "# generate_sgd_figure('MSELoss', 'exponential', wandb_records, fig_name='c', x ='optim_steps', y='grad_norm')\n",
    "# generate_sgd_figure('BCEWithLogitsLoss', 'exponential', wandb_records, fig_name='c', x ='optim_steps', y='grad_norm')\n",
    "# # time-elapsed comparison\n",
    "# generate_sgd_figure('MSELoss', 'exponential', wandb_records, fig_name='c', x ='time_elapsed', y='avg_loss')\n",
    "# generate_sgd_figure('BCEWithLogitsLoss', 'exponential', wandb_records, fig_name='c', x ='time_elapsed', y='avg_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43196d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "  wandb_records = pd.read_csv('/home/wlavington/Desktop/FunctionalStochasticOptimization/plotting/logs/wandb_data/__full__aistats_all.csv', header=0, squeeze=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4555686",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    " def generate_runtime_figure(loss, schedule, wandb_records, fig_name):\n",
    "    \n",
    "    # base info   \n",
    "    x ='optim_steps'\n",
    "    y ='time_elapsed'\n",
    "    \n",
    "       # base info   \n",
    "    dataset_names = ['rcv1',]\n",
    "    batch_sizes = [25, 125, 625]\n",
    "    m = [1, 5, 10, 20] \n",
    "#     wandb_records = wandb_records[wandb_records['group']=='AIstats_narval']\n",
    "    # init plots \n",
    "    fig, axs = plt.subplots(len(dataset_names), len(batch_sizes)+1, figsize=(16, 2))\n",
    "    axs = [axs]\n",
    "    colors = mpl.cm.Set1.colors   # Qualitative colormap\n",
    "    colormap = {'SGD': '#44AA99' , 'SLS': '#DDCC77', 'Adam': '#88CCEE'}\n",
    "    colormap.update({'SSO-1':  '#CC6677' ,  'SSO-5': '#AA4499', 'SSO-10': '#882255' , 'SSO-100': '#332288',  'SSO-20': '#332288'})\n",
    "    algorithms = ['SGD', 'Adam', 'SLS'] + ['SSO-'+str(m_) for m_ in m] \n",
    "    label_map = {x:'Optimization-Steps', y:'Gradient-Norm'}\n",
    "    \n",
    "    # now add in the lines to each of the plots \n",
    "    for row, dataset_name in enumerate(dataset_names):\n",
    "        \n",
    "        # figure out axis automatically  \n",
    "        run_times = []\n",
    "        run_times_min = []\n",
    "        run_times_max = []\n",
    "        algos = []\n",
    "\n",
    "        # SGD\n",
    "        proc_df = format_dataframe(wandb_records,\n",
    "            id_subfields={'fullbatch': 1,  \n",
    "            'use_optimal_stepsize': 1, 'loss': loss, 'algo': 'SGD',\n",
    "            'eta_schedule': schedule, 'dataset_name': dataset_name},\n",
    "            x_col=x , y_col=y) \n",
    "        if proc_df is not None:\n",
    "            run_times.append(proc_df[y].values.max())\n",
    "            run_times_max.append(proc_df[y+'75'].values.max())\n",
    "            run_times_min.append(proc_df[y+'25'].values.max())\n",
    "            algos.append('SGD')\n",
    "        else:\n",
    "            run_times.append(0.)\n",
    "            run_times_max.append(0.)\n",
    "            run_times_min.append(0.)\n",
    "            algos.append('SGD')\n",
    "            print('missing SLS  ', dataset_name, 'full-batch')\n",
    "        \n",
    "        # Adam\n",
    "#         proc_df = format_dataframe(wandb_records,\n",
    "#             id_subfields={'fullbatch': 1, #'_step': 499.0,  \n",
    "#             'use_optimal_stepsize': 1, 'loss': loss, 'algo': 'Adam',\n",
    "#             'eta_schedule': 'constant', 'dataset_name': dataset_name},\n",
    "#             x_col=x , y_col=y) \n",
    "#         if proc_df is not None:\n",
    "#             run_times.append(proc_df[y].values.max())\n",
    "#             run_times_max.append(proc_df[y+'75'].values.max())\n",
    "#             run_times_min.append(proc_df[y+'25'].values.max())\n",
    "#             algos.append('Adam')\n",
    "#         else:\n",
    "#             run_times.append(0.)\n",
    "#             run_times_max.append(0.)\n",
    "#             run_times_min.append(0.)\n",
    "#             algos.append('Adam')\n",
    "#             print('missing Adam  ', dataset_name, 'full-batch')\n",
    "        \n",
    "        # SLS\n",
    "        proc_df = format_dataframe(wandb_records,\n",
    "            id_subfields={'fullbatch': 1,  \n",
    "            'use_optimal_stepsize': 1, 'loss': loss, 'algo': 'LSOpt',\n",
    "            'eta_schedule': schedule, 'dataset_name': dataset_name},\n",
    "            x_col=x , y_col=y)  \n",
    "        if proc_df is not None:\n",
    "            run_times.append(proc_df[y].values.max())\n",
    "            run_times_max.append(proc_df[y+'75'].values.max())\n",
    "            run_times_min.append(proc_df[y+'25'].values.max())\n",
    "            algos.append('SLS')\n",
    "        else:\n",
    "            run_times.append(0.)\n",
    "            run_times_max.append(0.)\n",
    "            run_times_min.append(0.)\n",
    "            algos.append('SLS')\n",
    "            print('missing SLS  ', dataset_name, 'full-batch')\n",
    "\n",
    "        # FMDopt theoretical \n",
    "        for m_ in m:\n",
    "            # create parsed info \n",
    "            proc_df = format_dataframe(wandb_records, \n",
    "                id_subfields={'fullbatch': 1,   \n",
    "                    'use_optimal_stepsize': 1, \n",
    "                    'loss': loss, 'algo': 'SGD_FMDOpt', 'm': m_,\n",
    "                    'eta_schedule': schedule, 'dataset_name': dataset_name}, \n",
    "                     avg_subfields=['seed'], max_subfields=['c'],\n",
    "                x_col=x, y_col=y)\n",
    "            if proc_df is not None:\n",
    "                run_times.append(proc_df[y].values.max())\n",
    "                run_times_max.append(proc_df[y+'75'].values.max())\n",
    "                run_times_min.append(proc_df[y+'25'].values.max())\n",
    "                algos.append('SSO-'+str(m_))\n",
    "            else:\n",
    "                run_times.append(0.)\n",
    "                run_times_max.append(0.)\n",
    "                run_times_min.append(0.)\n",
    "                algos.append('SSO-'+str(m_))\n",
    "        \n",
    "        axs[row][-1].barh([_ for _ in range(len(algos))], run_times, \n",
    "                                align='center', edgecolor = \"black\",\n",
    "                                color=[colormap[algo] for algo in algos],\n",
    "#                                   xerr=[run_times_min, run_times_max]\n",
    "                         )\n",
    "        axs[row][-1].set_yticks([_ for _ in range(len(algos))], labels=algos)\n",
    "        axs[row][-1].grid()      \n",
    "        \n",
    "        # mini-batch plots \n",
    "        for col, batch_size in enumerate(batch_sizes):\n",
    "            \n",
    "            # figure out axis automatically \n",
    "            run_times = []\n",
    "            run_times_min = []\n",
    "            run_times_max = []\n",
    "            algos = []\n",
    "            \n",
    "            # SGD\n",
    "            proc_df = format_dataframe(wandb_records,\n",
    "                id_subfields={'batch_size': batch_size, #'_step': 499.0,\n",
    "                'use_optimal_stepsize': 1, 'loss': loss, 'algo': 'SGD',\n",
    "                'eta_schedule': schedule, 'dataset_name': dataset_name},\n",
    "                x_col=x , y_col=y) \n",
    "            if proc_df is not None:\n",
    "                run_times.append(proc_df[y].values.max())\n",
    "                run_times_max.append(proc_df[y+'75'].values.max())\n",
    "                run_times_min.append(proc_df[y+'25'].values.max())\n",
    "                algos.append('SGD')\n",
    "            else:\n",
    "                run_times.append(0.)\n",
    "                run_times_max.append(0.)\n",
    "                run_times_min.append(0.)\n",
    "                algos.append('SGD')\n",
    "                print('missing SGD  ', dataset_name, 'full-batch')\n",
    "            \n",
    "            # Adam\n",
    "#             proc_df = format_dataframe(wandb_records,\n",
    "#                 id_subfields={'batch_size': batch_size, #'_step': 499.0,  \n",
    "#                 'use_optimal_stepsize': 1, 'loss': loss, 'algo': 'Adam',\n",
    "#                 'eta_schedule': 'constant', 'dataset_name': dataset_name},\n",
    "#                 x_col=x , y_col=y) \n",
    "#             if proc_df is not None:\n",
    "#                 run_times.append(proc_df[y].values.max())\n",
    "#                 run_times_max.append(proc_df[y+'75'].values.max())\n",
    "#                 run_times_min.append(proc_df[y+'25'].values.max())\n",
    "#                 algos.append('Adam')\n",
    "#             else:\n",
    "#                 run_times.append(0.)\n",
    "#                 run_times_max.append(0.)\n",
    "#                 run_times_min.append(0.)\n",
    "#                 algos.append('Adam')\n",
    "#                 print('missing Adam  ', dataset_name, 'full-batch')\n",
    "            \n",
    "            # SLS\n",
    "            proc_df = format_dataframe(wandb_records,\n",
    "                id_subfields={'batch_size': batch_size,  #'_step': 499.0,\n",
    "                'use_optimal_stepsize': 1, 'loss': loss, 'algo': 'LSOpt',\n",
    "                'eta_schedule': schedule, 'dataset_name': dataset_name},\n",
    "                x_col=x , y_col=y) \n",
    "            if proc_df is not None:\n",
    "                run_times.append(proc_df[y].values.max())\n",
    "                run_times_max.append(proc_df[y+'75'].values.max())\n",
    "                run_times_min.append(proc_df[y+'25'].values.max())\n",
    "                algos.append('SLS')\n",
    "            else:\n",
    "                run_times.append(0.)\n",
    "                run_times_max.append(0.)\n",
    "                run_times_min.append(0.)\n",
    "                algos.append('SLS')\n",
    "                print('missing SLS  ', dataset_name, 'full-batch')\n",
    "    \n",
    "            # FMDopt theoretical \n",
    "            for m_ in m:\n",
    "                # create parsed info \n",
    "                proc_df = format_dataframe(wandb_records, \n",
    "                    id_subfields={'batch_size': batch_size,  \n",
    "                        'use_optimal_stepsize': 1, #'_step': 499.0,\n",
    "                        'loss': loss, 'algo': 'SGD_FMDOpt', 'm': m_,\n",
    "                        'eta_schedule': schedule, 'dataset_name': dataset_name}, \n",
    "                         avg_subfields=['seed'], max_subfields=['c'],\n",
    "                    x_col=x, y_col=y)\n",
    "                if proc_df is not None:\n",
    "                    run_times.append(proc_df[y].values.max())\n",
    "                    run_times_max.append(proc_df[y+'75'].values.max())\n",
    "                    run_times_min.append(proc_df[y+'25'].values.max())\n",
    "                    algos.append('SSO-'+str(m_))\n",
    "                else:\n",
    "                    run_times.append(0.)\n",
    "                    run_times_max.append(0.)\n",
    "                    run_times_min.append(0.)\n",
    "                    algos.append('SSO-'+str(m_))\n",
    "            \n",
    "            axs[row][col].barh([_ for _ in range(len(algos))], run_times, \n",
    "                                align='center', edgecolor = \"black\",\n",
    "                                color=[colormap[algo] for algo in algos],\n",
    "#                                 xerr=[run_times_min, run_times_max]\n",
    "                              )\n",
    "            axs[row][col].set_yticks([_ for _ in range(len(algos))], labels=algos)\n",
    "#             axs[row][col].set_title('batch-size: '+str(batch_size), fontsize=22)\n",
    "            axs[row][col].grid()  \n",
    "        \n",
    "        # \n",
    "        \n",
    "#         axs[0][-1].set_title('full-batch', fontsize=22)\n",
    "#         axs[row][-1].set_ylabel(dataset_name, fontsize=22)\n",
    "        axs[row][-1].yaxis.set_label_position(\"right\") \n",
    "    \n",
    "#     plt.subplots_adjust(hspace=1.5)\n",
    "#     plt.rcParams['figure.dpi'] = 100# 400 \n",
    "    fig.tight_layout()\n",
    "    \n",
    "    # show / save\n",
    "    plt.savefig('plots/aistats/workshop-plot-'+fig_name+loss+'.pdf', bbox_inches='tight')\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaeddb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    " generate_runtime_figure('MSELoss', 'constant', wandb_records, fig_name='run-times')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272b208f",
   "metadata": {},
   "outputs": [],
   "source": [
    " def generate_sgd_figure(wandb_records, fig_name, x ='optim_steps', y='avg_loss'):\n",
    "    \n",
    "    # base info   \n",
    "    dataset_name = 'mushrooms' # , 'ijcnn', 'rcv1' \n",
    "    m = [1, 10, 100, 1000]\n",
    "    sso_variants = ['SLS_FMDOpt', 'Diag_Ada_FMDOpt'] #'Online_Newton_FMDOpt' 'SGD_FMDOpt',\n",
    "    loss = 'BCEWithLogitsLoss'\n",
    "    schedule = 'constant'\n",
    "#     wandb_records = wandb_records[wandb_records['group']=='AIstats_narval']\n",
    "    # init plots \n",
    "    fig, axs = plt.subplots(1, 2, figsize=(10, 3))\n",
    "    axs = [axs]\n",
    "    colors = mpl.cm.Set1.colors   # Qualitative colormap\n",
    "    colormap = {'SGD': '#44AA99' , 'SLS': '#DDCC77', 'Adagrad': '#88CCEE'}\n",
    "    colormap.update({'SSO-1':  '#CC6677' ,  'SSO-10': '#AA4499', 'SSO-100': '#882255', 'SSO-1000': '#332288'})\n",
    "    algorithms = ['SGD', 'Adagrad', 'SLS'] + ['SSO-'+str(m_) for m_ in m] \n",
    "    label_map = {x:'Optimization-Steps', y:'Gradient-Norm'}\n",
    "    algo_mask = {'SGD_FMDOpt': 'SSO-SGD', 'SLS_FMDOpt': 'SSO-SLS', \n",
    "                 'Diag_Ada_FMDOpt': 'SSO-Adagrad', \n",
    "                 'Online_Newton_FMDOpt': 'SSO-Newton'}\n",
    "    # now add in the lines to each of the plots \n",
    "    for col, sso_variant in enumerate(sso_variants):\n",
    "        \n",
    "        row = min(int(np.floor(col / 2)), 1)\n",
    "        col = col % 2\n",
    "        # figure out axis automatically \n",
    "        x_max = 0 \n",
    "\n",
    "        # SLS\n",
    "        proc_df = format_dataframe(wandb_records,\n",
    "            id_subfields={'fullbatch': 1,  \n",
    "            'use_optimal_stepsize': 1, 'loss': loss, 'algo': 'LSOpt',\n",
    "            'eta_schedule': schedule, 'dataset_name': dataset_name},\n",
    "            x_col=x , y_col=y, k=3) \n",
    "        if proc_df is not None:\n",
    "            x_max = max(proc_df[x].values.max(), x_max)\n",
    "            axs[row][col] = generate_plot(proc_df, x, y, axs[row][col], label='SLS', \n",
    "                                         linestyle='dashed', color=colormap['SLS'])\n",
    "        else:\n",
    "            print('missing SLS  ', dataset_name, 'full-batch')\n",
    "\n",
    "        # SGD\n",
    "        proc_df = format_dataframe(wandb_records,\n",
    "            id_subfields={'fullbatch': 1,  \n",
    "            'use_optimal_stepsize': 1, 'loss': loss, 'algo': 'SGD',\n",
    "            'eta_schedule': schedule, 'dataset_name': dataset_name},\n",
    "            x_col=x , y_col=y, k=3)\n",
    "        if proc_df is not None: \n",
    "            x_max = max(proc_df[x].values.max(), x_max)\n",
    "            axs[row][col] = generate_plot(proc_df, x, y, axs[row][col], label='SGD', \n",
    "                                         linestyle='dashed', color=colormap['SGD'])\n",
    "        else:\n",
    "            print('missing SGD  ', dataset_name, 'full-batch')\n",
    "        \n",
    "        # Adagrad\n",
    "        proc_df = format_dataframe(wandb_records,\n",
    "            id_subfields={'fullbatch': 1,  \n",
    "            'use_optimal_stepsize': 1, 'loss': loss, 'algo': 'Adagrad',\n",
    "            'eta_schedule': 'constant', 'dataset_name': dataset_name},\n",
    "            x_col=x , y_col=y, k=2) \n",
    "        if proc_df is not None: \n",
    "            x_max = max(proc_df[x].values.max(), x_max)\n",
    "            axs[row][col] = generate_plot(proc_df, x, y, axs[row][col], label='Adagrad', \n",
    "                                         linestyle='dashed', color=colormap['Adagrad'])\n",
    "        else:\n",
    "            print('missing Adagrad  ', dataset_name, 'full-batch')\n",
    "        \n",
    "        # FMDopt theoretical \n",
    "        for m_ in m:\n",
    "            # create parsed info \n",
    "            proc_df = format_dataframe(wandb_records, \n",
    "                id_subfields={'fullbatch': 1,   \n",
    "                    'use_optimal_stepsize': 1, \n",
    "                    'loss': loss, 'algo': sso_variant, 'm': m_,\n",
    "                    'eta_schedule': schedule, 'dataset_name': dataset_name}, \n",
    "                     avg_subfields=['seed'], max_subfields=['c'],\n",
    "                x_col=x, y_col=y, k=3)\n",
    "            if proc_df is not None:\n",
    "                x_max = max(proc_df[x].values.max(), x_max)\n",
    "                axs[row][col] = generate_plot(proc_df, x, y, axs[row][col],  \\\n",
    "                                        label='SSO-'+str(m_), linestyle='solid', color=colormap['SSO-'+str(m_)])\n",
    "            else:\n",
    "                print('missing '+sso_variant+'FMDopt  ', m_, dataset_name, 'full-batch') \n",
    "        axs[row][col].grid()     \n",
    "        axs[row][col].set_yscale(\"log\")\n",
    "        axs[row][col].set_xscale(\"log\")  \n",
    "#         axs[row][col]..set_ylim([1e-6, ymax])\n",
    "#         axs[0][col].ticklabel_format(axis='x', style='sci', scilimits=(0,0))\n",
    "#         axs[0][col].yaxis.set_major_locator(plt.MaxNLocator(4)) \n",
    "#         axs[0][col].set_title('batch-size: '+str(batch_size), fontsize=22)\n",
    "        axs[row][col].set_title(algo_mask[sso_variant], fontsize=18)\n",
    "#         axs[0][col].set_ylabel(dataset_name, fontsize=22)\n",
    "#         axs[0][col].yaxis.set_label_position(\"right\") \n",
    "#         axs[0][col].xaxis.set_minor_locator(mpl.ticker.LogLocator(base=10,numticks=100))\n",
    "#         axs[0][col].xaxis.set_minor_formatter(mpl.ticker.NullFormatter()) \n",
    "#         axs[0][col].yaxis.set_minor_formatter(mpl.ticker.NullFormatter())\n",
    "        \n",
    "#     remaining format stuff  \n",
    "    handles = [mpatches.Patch(color=colormap[algo], label=algo) for algo in algorithms]\n",
    "    leg = fig.legend(handles=handles,\n",
    "           loc=\"lower center\",   # Position of legend\n",
    "           borderaxespad=1.25,    # Small spacing around legend box\n",
    "           # title=\"Algorithms\",  # Title for the legend\n",
    "           fontsize=12,\n",
    "           ncol=8, \n",
    "           bbox_to_anchor=(0.525, -0.175),\n",
    "           )\n",
    "    \n",
    "    plt.subplots_adjust(hspace=1.25)\n",
    "    plt.rcParams['figure.dpi'] = 100# 400 \n",
    "    fig.tight_layout()\n",
    "    \n",
    "    # show / save\n",
    "    plt.savefig('plots/aistats/workshop-plot-algo-compare-'+fig_name+loss+'.pdf', bbox_inches='tight')\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bc88a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb_records = pd.read_csv('logs/wandb_data/__full__'+SUMMARY_FILE, header=0, squeeze=True)\n",
    "generate_sgd_figure(wandb_records, fig_name='other-optim-ex', x ='optim_steps', y='avg_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40095e4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febbd3df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bcbf64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
