{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6817e1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import wandb\n",
    "api = wandb.Api(timeout=180)\n",
    "import os\n",
    "import pandas as pd\n",
    "import wandb\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "from copy import deepcopy\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import argparse\n",
    "import itertools\n",
    "import  matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import itertools\n",
    "import time\n",
    "import matplotlib as mpl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "03fa6653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists\n"
     ]
    }
   ],
   "source": [
    "USER='wilderlavington'\n",
    "PROJECT='FunctionalOptimization'\n",
    "SUMMARY_FILE='sharan_report_0923.csv'\n",
    "EPISODES = 25000\n",
    "K=2\n",
    "# make plots dir\n",
    "try:\n",
    "    os.makedirs(\"plots/0923/\")\n",
    "except FileExistsError:\n",
    "    print(\"File already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6745cfe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_wandb_summary(sweeps=None, ignore_git_commits=['dab417057ac964ae2244384aeb9e51f0475062ae']):\n",
    "    \"\"\"\n",
    "    Download a summary of all runs on the wandb project\n",
    "    \"\"\"\n",
    "    runs = api.runs(USER+'/'+PROJECT, per_page=1000000000)\n",
    "    summary_list = []\n",
    "    config_list = []\n",
    "    name_list = []\n",
    "    id_list = []\n",
    "    commits = []\n",
    "    assert len([run for run in runs]) \n",
    "    for run in tqdm(runs):\n",
    "        commits.append(run.commit)\n",
    "        if ignore_git_commits is None:\n",
    "            if (sweeps is not None):\n",
    "                if (run.sweep is not None) and (run.sweep.id in sweeps):\n",
    "                    summary_list.append(run.summary._json_dict)\n",
    "                    run = api.run(USER+'/'+PROJECT+\"/\"+run.id)\n",
    "                    config_list.append({k: v for k, v in run.config.items()})\n",
    "                    name_list.append(run.name)\n",
    "                    id_list.append(run.id)\n",
    "            elif sweeps is None:\n",
    "                summary_list.append(run.summary._json_dict)\n",
    "                run = api.run(USER+'/'+PROJECT+\"/\"+run.id)\n",
    "                config_list.append({k: v for k, v in run.config.items()})\n",
    "                name_list.append(run.name)\n",
    "                id_list.append(run.id)\n",
    "            else:\n",
    "                pass\n",
    "        else:\n",
    "            if run.commit in ignore_git_commits:\n",
    "                pass\n",
    "            else:\n",
    "                if (sweeps is not None):\n",
    "                    if (run.sweep is not None) and (run.sweep.id in sweeps):\n",
    "                        summary_list.append(run.summary._json_dict)\n",
    "                        run = api.run(USER+'/'+PROJECT+\"/\"+run.id)\n",
    "                        config_list.append({k: v for k, v in run.config.items()})\n",
    "                        name_list.append(run.name)\n",
    "                        id_list.append(run.id)\n",
    "                elif sweeps is None:\n",
    "                    summary_list.append(run.summary._json_dict)\n",
    "                    run = api.run(USER+'/'+PROJECT+\"/\"+run.id)\n",
    "                    config_list.append({k: v for k, v in run.config.items()})\n",
    "                    name_list.append(run.name)\n",
    "                    id_list.append(run.id)\n",
    "                else:\n",
    "                    pass\n",
    "    print(set(commits))\n",
    "    commits_df = pd.DataFrame.from_records(commits)\n",
    "    summary_df = pd.DataFrame.from_records(summary_list)\n",
    "    config_df = pd.DataFrame.from_records(config_list)\n",
    "    name_df = pd.DataFrame({\"name\": name_list, \"id\": id_list})\n",
    "    all_df = pd.concat([name_df, config_df, summary_df, commits_df], axis=1)\n",
    "    Path('logs/wandb_data/').mkdir(parents=True, exist_ok=True)\n",
    "    all_df.to_csv('logs/wandb_data/'+SUMMARY_FILE)\n",
    "\n",
    "def download_wandb_records():\n",
    "    \"\"\"\n",
    "    Download data for all runs in summary file\n",
    "    \"\"\"\n",
    "    # load it all in and clean it up\n",
    "    runs_df = pd.read_csv('logs/wandb_data/'+SUMMARY_FILE, header=0, squeeze=True)\n",
    "    runs_df = runs_df.loc[:,~runs_df.columns.duplicated()]\n",
    "    columns_of_interest = ['avg_loss', 'optim_steps', 'grad_norm', 'time_elapsed', \\\n",
    "             'grad_evals', 'function_evals']\n",
    "    # set which columns we will store for vizualization\n",
    "    list_of_dataframes = []\n",
    "    # iterate through all runs to create individual databases\n",
    "    for ex in tqdm(range(len(runs_df)), leave=False):\n",
    "        # get the associated runs\n",
    "        run = api.run(USER+'/'+PROJECT+'/'+runs_df.loc[runs_df.iloc[ex,0],:]['id'])\n",
    "        run_df = []\n",
    "        # iterate through all rows in online database\n",
    "        base_info = {}\n",
    "        for key in runs_df.loc[runs_df.iloc[ex,0],:].keys():\n",
    "            base_info.update({key:runs_df.loc[runs_df.iloc[ex,0],:][key]})\n",
    "        for i, row in run.history().iterrows():\n",
    "            row_info = deepcopy(base_info)\n",
    "            row_info.update({key:row[key] for key in columns_of_interest if key in row.keys()})\n",
    "            run_df.append(row_info)\n",
    "        # convert format to dataframe and add to our list\n",
    "        list_of_dataframes.append(pd.DataFrame(run_df))\n",
    "    # combine and then store\n",
    "    wandb_records = pd.concat(list_of_dataframes)\n",
    "    wandb_records.to_csv('logs/wandb_data/__full__'+SUMMARY_FILE)\n",
    "    # return single data frame for vizualization\n",
    "    return wandb_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5a7f7a21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can only concatenate str (not \"float\") to str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_73916/8259246.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# download_wandb_summary()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdownload_wandb_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_73916/3157811747.py\u001b[0m in \u001b[0;36mdownload_wandb_records\u001b[0;34m()\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mruns_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleave\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;31m# get the associated runs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0mrun\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mUSER\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mPROJECT\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mruns_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mruns_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0mrun_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;31m# iterate through all rows in online database\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: can only concatenate str (not \"float\") to str"
     ]
    }
   ],
   "source": [
    "# download_wandb_summary()\n",
    "download_wandb_records()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e8ea348d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth(array, k):\n",
    "    array = np.array(array)\n",
    "    new_array = deepcopy(array)\n",
    "    # print(array[max(0,i-k):i] )\n",
    "    for i in range(len(array)):\n",
    "        if str(array[i]) != 'nan':\n",
    "            avg_list = [val for val in array[max(0,i-k):i+1] if str(val) != 'nan']\n",
    "            new_array[i] = sum(avg_list) / len(avg_list)\n",
    "    return new_array\n",
    "def format_dataframe(records, id_subfields={}, avg_subfields=['seed'],\n",
    "            max_subfields=['log_eta', 'eta_schedule', 'c'],\n",
    "            x_col='optim_steps', y_col='avg_loss'):\n",
    "    #\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    max_subfields = [m for m in max_subfields if m not in id_subfields.keys()]\n",
    "\n",
    "    for key in id_subfields: \n",
    "        records = records.loc[records[key] == id_subfields[key]] \n",
    "    records['function_evals+grad_evals'] = records['function_evals']+records['grad_evals']\n",
    "    if not len(records):\n",
    "        return None\n",
    "    # remove nans\n",
    "    records = records[records[y_col].notna()]\n",
    "    important_cols = list(set(avg_subfields+max_subfields+\\\n",
    "        list(id_subfields.keys())+[x_col, y_col, 'optim_steps']))\n",
    "    # remove redundant information\n",
    "    records = records[important_cols]\n",
    "    # average over avg_subfields\n",
    "    records = records.drop(avg_subfields, axis=1)\n",
    "    # group over averaging field\n",
    "    gb = list(set(list(max_subfields+list(id_subfields.keys())+[x_col, 'optim_steps'])))\n",
    "    # only look at final optim steps\n",
    "    last_mean_records = records.loc[records['optim_steps'] == records['optim_steps'].max()]\n",
    "    # get the best record\n",
    "    best_record = last_mean_records[last_mean_records[y_col] == last_mean_records[y_col].min()]\n",
    "    # find parameters of the best record\n",
    "    merge_on = list(set(gb)-set(['optim_steps', x_col, y_col]))\n",
    "    merge_on = [ x for x in merge_on if x in best_record.columns.values]\n",
    "    best_records = pd.merge(best_record[merge_on], records, on=merge_on,how='left')\n",
    "    final_records = best_records.groupby(merge_on+[x_col], as_index=False)[y_col].mean()\n",
    "    final_records[y_col+'25'] = best_records.groupby(merge_on+[x_col], as_index=False)[y_col].quantile(0.25)[y_col]\n",
    "    final_records[y_col+'75'] = best_records.groupby(merge_on+[x_col], as_index=False)[y_col].quantile(0.75)[y_col]\n",
    "    final_records = final_records.sort_values(x_col, axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last')\n",
    "    # smooth outputs \n",
    "    final_records[y_col+'75'] = smooth(final_records[y_col+'75'],K)\n",
    "    final_records[y_col+'25'] = smooth(final_records[y_col+'25'],K)\n",
    "    final_records[y_col] = smooth(final_records[y_col],K) \n",
    "    # return \n",
    "    return final_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b5b7836a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_plot(proc_df, x, y, ax, label, linestyle='solid', color=None, x_max=100000):\n",
    "    low_order_idx = (torch.tensor(proc_df[x].values) < x_max).nonzero().reshape(-1)\n",
    "    if label:\n",
    "        ax.plot(torch.tensor(proc_df[x].values[low_order_idx]), \n",
    "                torch.tensor(proc_df[y].values[low_order_idx]), \n",
    "                label=label, linestyle=linestyle, color=color,\n",
    "                linewidth=4)\n",
    "    else:\n",
    "        ax.plot(torch.tensor(proc_df[x].values[low_order_idx]), \n",
    "                torch.tensor(proc_df[y].values[low_order_idx]), \n",
    "                label='_nolegend_', linestyle=linestyle, color=color,\n",
    "                linewidth=4)\n",
    "    ax.fill_between(torch.tensor(proc_df[x].values)[low_order_idx],\n",
    "            torch.tensor(proc_df[y+'75'].values)[low_order_idx],\n",
    "            torch.tensor(proc_df[y+'25'].values)[low_order_idx],\n",
    "            alpha = 0.5, label='_nolegend_', linestyle=linestyle, color=color)\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "cd9f1a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_A1_figure(loss, dataset_name, wandb_records):\n",
    "    \n",
    "    # base info   \n",
    "    schedules = ['constant', 'stochastic', 'exponential']\n",
    "    batch_sizes = [5, 25, 125, 625]\n",
    "    m = [1, 2, 5, 10, 20]\n",
    "    x = 'optim_steps'\n",
    "    y = 'grad_norm'\n",
    "    \n",
    "    # init plots \n",
    "    fig, axs = plt.subplots(len(schedules), len(batch_sizes), figsize=(21, 21), sharey=True)\n",
    "    colors = mpl.cm.Set1.colors   # Qualitative colormap\n",
    "    colormap = {'SGD':colors[0], 'SLS':colors[1]}\n",
    "    colormap.update({'FuncOpt'+str(m_):colors[idx+2] for idx, m_ in enumerate(m)})\n",
    "    algorithms = ['SGD', 'SLS'] + ['FuncOpt'+str(m_) for m_ in m]\n",
    "    plt.title('Comparison of SGD/SLS/FuncOpt: '+loss+'-'+dataset_name)\n",
    "    label_map = {x:'Time-Elapsed', y:'Gradient-Norm'}\n",
    "    \n",
    "    # now add in the lines to each of the plots \n",
    "    for row, schedule in enumerate(schedules):\n",
    "        \n",
    "        # mini-batch plots \n",
    "        for col, batch_size in enumerate(batch_sizes):\n",
    "            \n",
    "            # figure out axis automatically \n",
    "            x_max = 0\n",
    "            \n",
    "            # SLS\n",
    "            proc_df = format_dataframe(wandb_records,\n",
    "                id_subfields={'batch_size': batch_size, 'episodes': EPISODES,\n",
    "                'use_optimal_stepsize': 1, 'loss': loss, 'algo': 'LSOpt',\n",
    "                'eta_schedule': schedule, 'dataset_name': dataset_name},\n",
    "                x_col=x , y_col=y)\n",
    "            if proc_df is not None:\n",
    "                x_max = max(proc_df[x].values.max(), x_max)\n",
    "                axs[row][col] = generate_plot(proc_df, x, y, axs[row][col], label='SLS', \n",
    "                                             linestyle='solid', color=colormap['SLS'])\n",
    "            else:\n",
    "                print('missing SLS  ', schedule, batch_size)\n",
    "            \n",
    "            # SGD\n",
    "            proc_df = format_dataframe(wandb_records,\n",
    "                id_subfields={'batch_size': batch_size, 'episodes': EPISODES,\n",
    "                'use_optimal_stepsize': 1, 'loss': loss, 'algo': 'SGD',\n",
    "                'eta_schedule': schedule, 'dataset_name': dataset_name},\n",
    "                x_col=x , y_col=y)\n",
    "            if proc_df is not None: \n",
    "                x_max = max(proc_df[x].values.max(), x_max)\n",
    "                axs[row][col] = generate_plot(proc_df, x, y, axs[row][col], label='SGD', \n",
    "                                             linestyle='solid', color=colormap['SGD'])\n",
    "            else:\n",
    "                print('missing SGD  ', schedule, batch_size)\n",
    "    \n",
    "            # FMDopt theoretical \n",
    "            for m_ in m:\n",
    "                # create parsed info \n",
    "                proc_df = format_dataframe(wandb_records, \n",
    "                    id_subfields={'batch_size': batch_size, 'episodes': EPISODES,\n",
    "                        'use_optimal_stepsize': 0, 'log_eta': 0.,\n",
    "                        'loss': loss, 'algo': 'SGD_FMDOpt', 'm': m_,\n",
    "                        'eta_schedule': schedule, 'dataset_name': dataset_name}, \n",
    "                         avg_subfields=['seed'], max_subfields=['c'],\n",
    "                    x_col=x, y_col=y)\n",
    "                x_max = max(proc_df[x].values.max(), x_max)\n",
    "                if proc_df is not None:\n",
    "                    axs[row][col] = generate_plot(proc_df, x, y, axs[row][col],  \\\n",
    "                                            label='FuncOpt'+str(m_), linestyle='solid', color=colormap['FuncOpt'+str(m_)])\n",
    "                else:\n",
    "                    print('missing FMDopt  ', m_, schedule, batch_size)\n",
    "            \n",
    "            axs[row][col].set_xlim(200, x_max)  \n",
    "            axs[row][col].grid()    \n",
    "            axs[row][col].set_title('schedule: '+schedule+', batch_size: '+str(batch_size), fontsize=18)\n",
    "            axs[row][col].set_xlabel(label_map[x], fontsize=16)\n",
    "            axs[row][col].set_ylabel(label_map[y], fontsize=16)\n",
    "            axs[row][col].tick_params(axis='both', which='major', labelsize=14)\n",
    "            axs[row][col].tick_params(axis='both', which='minor', labelsize=14)\n",
    "    \n",
    "    # remaining format stuff  \n",
    "    handles = [mpatches.Patch(color=colormap[algo], label=algo) for algo in algorithms]\n",
    "    leg = fig.legend(handles=handles,\n",
    "           loc=\"lower center\",   # Position of legend\n",
    "           borderaxespad=1.65,    # Small spacing around legend box\n",
    "           # title=\"Algorithms\",  # Title for the legend\n",
    "           fontsize=18,\n",
    "           ncol=7, \n",
    "           bbox_to_anchor=(0.5, -0.05),\n",
    "           )\n",
    "    plt.yscale(\"log\")\n",
    "    plt.subplots_adjust(hspace=1.5)\n",
    "    plt.rcParams['figure.dpi'] = 400 \n",
    "    plt.suptitle('Comparison of SGD/SLS/FuncOpt: Loss: '+loss+', Dataset: '+dataset_name, fontsize=28)\n",
    "    fig.tight_layout()\n",
    "    fig.subplots_adjust(top=0.95)\n",
    "    \n",
    "    # show / save\n",
    "    plt.savefig('plots/0902/'+loss+'_'+dataset_name+'.pdf', bbox_inches='tight')\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f4555686",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'logs/wandb_data/__full__sharan_report_0923.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_73916/2238643779.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdata_sets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'mushrooms'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ijcnn'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rcv1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'MSELoss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'BCEWithLogitsLoss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mwandb_records\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'logs/wandb_data/__full__'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mSUMMARY_FILE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msqueeze\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdata_set\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_sets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/mujoco_env/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    608\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/mujoco_env/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/mujoco_env/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    817\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/mujoco_env/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1048\u001b[0m             )\n\u001b[1;32m   1049\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1050\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1051\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1052\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/mujoco_env/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1866\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1867\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1868\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1869\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"encoding\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"compression\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/mujoco_env/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m   1360\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHanldes\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m         \"\"\"\n\u001b[0;32m-> 1362\u001b[0;31m         self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1363\u001b[0m             \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m             \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/mujoco_env/lib/python3.8/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    640\u001b[0m                 \u001b[0merrors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"replace\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 642\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    643\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'logs/wandb_data/__full__sharan_report_0923.csv'"
     ]
    }
   ],
   "source": [
    "data_sets = ['mushrooms', 'ijcnn', 'rcv1']\n",
    "losses = ['MSELoss', 'BCEWithLogitsLoss']\n",
    "wandb_records = pd.read_csv('logs/wandb_data/__full__'+SUMMARY_FILE, header=0, squeeze=True)\n",
    " \n",
    "for data_set in data_sets:\n",
    "    for loss in losses:\n",
    "        print('generating SGD plot for ', data_set, loss)\n",
    "        generate_A1_figure(loss, data_set, wandb_records) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaeddb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_A2_figure(loss, dataset_name, wandb_records):\n",
    "    \n",
    "    # base info   \n",
    "    schedules = ['constant']\n",
    "    batch_sizes = [5, 25, 125, 625]\n",
    "    m = [1, 2, 5, 10, 20]\n",
    "    x = 'grad_evals'\n",
    "    y = 'grad_norm'\n",
    "    \n",
    "    # init plots \n",
    "    fig, axs = plt.subplots(len(schedules), len(batch_sizes), figsize=(21, 7), sharey=True)\n",
    "    colors = mpl.cm.Set1.colors   # Qualitative colormap\n",
    "    colormap = {'Adagrad':colors[0]}\n",
    "    colormap.update({'AdaFuncOpt'+str(m_):colors[idx+2] for idx, m_ in enumerate(m)})\n",
    "    axs = [axs]\n",
    "    label_map = {x:'Time-Elapsed', y:'Gradient-Norm'}\n",
    "    algorithms = ['Adagrad'] + ['AdaFuncOpt'+str(m_) for m_ in m]\n",
    "    \n",
    "    # now add in the lines to each of the plots \n",
    "    for row, schedule in enumerate(schedules):\n",
    "        for col, batch_size in enumerate(batch_sizes):\n",
    "            x_max = 0\n",
    "            \n",
    "#             # adagrad\n",
    "#             proc_df = format_dataframe(wandb_records,\n",
    "#                 id_subfields={'batch_size': batch_size, 'episodes': EPISODES,\n",
    "#                 'use_optimal_stepsize': 1, 'loss': loss, 'algo': 'Adagrad',\n",
    "#                 'eta_schedule': schedule, 'dataset_name': dataset_name},\n",
    "#                 x_col=x , y_col=y)\n",
    "            \n",
    "#             if proc_df is not None: \n",
    "#                 x_max = max(proc_df[x].values.max(), x_max)\n",
    "#                 axs[row][col] = generate_plot(proc_df, x, y, axs[row][col], label='Adagrad', \n",
    "#                                              linestyle='solid', color=colormap['Adagrad'])\n",
    "#             else:\n",
    "#                 print('missing adagrad  ', schedule, batch_size)\n",
    "            \n",
    "            # adagrad\n",
    "#             proc_df = format_dataframe(wandb_records,\n",
    "#                 id_subfields={'batch_size': batch_size, 'episodes': EPISODES,\n",
    "#                 'use_optimal_stepsize': 1, 'loss': loss, 'algo': 'Sadagrad',\n",
    "#                 'eta_schedule': schedule, 'dataset_name': dataset_name},\n",
    "#                 x_col=x , y_col=y)\n",
    "#             if proc_df is not None: \n",
    "#                 x_max = max(proc_df[x].values.max(), x_max)\n",
    "#                 axs[row][col] = generate_plot(proc_df, x, y, axs[row][col], label='Sadagrad', \n",
    "#                                              linestyle='solid', color=colormap['Adagrad'])\n",
    "#             else:\n",
    "#                 print('missing sadagrad  ', schedule, batch_size)\n",
    "            \n",
    "             # adagrad -- gridsearched\n",
    "            proc_df = format_dataframe(wandb_records,\n",
    "                id_subfields={'batch_size': batch_size, 'episodes': EPISODES,\n",
    "                'use_optimal_stepsize': 0, 'loss': loss, 'algo': 'Sadagrad',\n",
    "                              'log_eta': -2., \n",
    "                'eta_schedule': schedule, 'dataset_name': dataset_name},\n",
    "                x_col=x , y_col=y)\n",
    "            if proc_df is not None: \n",
    "                x_max = max(proc_df[x].values.max(), x_max)\n",
    "                axs[row][col] = generate_plot(proc_df, x, y, axs[row][col], label='Sadagrad', \n",
    "                                             linestyle='dotted', color=colormap['Adagrad'])\n",
    "            else:\n",
    "                print('missing sadagrad  ', schedule, batch_size)\n",
    "                    \n",
    "#             # AdaFMDopt theoretical \n",
    "#             for m_ in m:\n",
    "#                 # create parsed info \n",
    "#                 proc_df = format_dataframe(wandb_records, \n",
    "#                     id_subfields={'batch_size': batch_size, 'episodes': EPISODES,\n",
    "#                         'use_optimal_stepsize': 0, 'log_eta': -3,\n",
    "#                         'loss': loss, 'algo': 'Ada_FMDOpt', 'm': m_,\n",
    "#                         'eta_schedule': schedule, 'dataset_name': dataset_name}, \n",
    "#                          avg_subfields=['seed'], max_subfields=['c'],\n",
    "#                     x_col=x, y_col=y)\n",
    "                \n",
    "#                 # generate the associated plot \n",
    "#                 if proc_df is not None:\n",
    "#                     axs[row][col] = generate_plot(proc_df, x, y, axs[row][col],  \\\n",
    "#                                             label='AdaFuncOpt', linestyle='solid', color=colormap['AdaFuncOpt'+str(m_)])\n",
    "#                 else:\n",
    "#                     print('missing AdaFMDopt  ', schedule, batch_size)\n",
    "                    \n",
    "            # AdaFMDopt grid_searched  \n",
    "            for m_ in m:\n",
    "                # create parsed info \n",
    "                proc_df = format_dataframe(wandb_records, \n",
    "                    id_subfields={'batch_size': batch_size, 'episodes': EPISODES,\n",
    "                        'use_optimal_stepsize': 0, 'log_eta': 0., \n",
    "                        'loss': loss, 'algo': 'Ada_FMDOpt', 'm': m_,\n",
    "                        'eta_schedule': schedule, 'dataset_name': dataset_name}, \n",
    "                         avg_subfields=['seed'], max_subfields=['log_eta', 'c'],\n",
    "                     x_col=x, y_col=y)\n",
    "                if proc_df is not None:\n",
    "                    axs[row][col] = generate_plot(proc_df, x, y, axs[row][col], label='Ada_FMDOpt', \n",
    "                           linestyle='dotted', color=colormap['AdaFuncOpt'+str(m_)])\n",
    "                else:\n",
    "                    print('missing AdaFMDopt  grid_searched', schedule, batch_size)\n",
    "            \n",
    "            axs[row][col].set_xlim(0, x_max)  \n",
    "            axs[row][col].grid()    \n",
    "            axs[row][col].set_title('schedule: '+schedule+', batch_size: '+str(batch_size), fontsize=20)\n",
    "            axs[row][col].set_xlabel(label_map[x], fontsize=16)\n",
    "            axs[row][col].set_ylabel(label_map[y], fontsize=16)\n",
    "            axs[row][col].tick_params(axis='both', which='major', labelsize=14)\n",
    "            axs[row][col].tick_params(axis='both', which='minor', labelsize=14)\n",
    "    \n",
    "    # remaining format stuff \n",
    "    handles = [mpatches.Patch(color=colormap[algo], label=algo) for algo in algorithms]\n",
    "    leg = fig.legend(handles=handles,\n",
    "           loc=\"lower center\",   # Position of legend\n",
    "           borderaxespad=1.65,    # Small spacing around legend box\n",
    "           # title=\"Algorithms\",  # Title for the legend\n",
    "           fontsize=18,\n",
    "           ncol=6, \n",
    "           bbox_to_anchor=(0.5, -0.15),\n",
    "           ) \n",
    "    plt.yscale(\"log\")\n",
    "    plt.subplots_adjust(hspace=1.25)\n",
    "    plt.rcParams['figure.dpi'] = 400 \n",
    "    plt.suptitle('Comparison of Adagrad/Ada-FuncOpt: Loss: '+loss+', Dataset: '+dataset_name, fontsize=28)\n",
    "    fig.tight_layout()\n",
    "    fig.subplots_adjust(top=0.85)\n",
    "    \n",
    "    # show / save\n",
    "    plt.savefig('plots/0902/Adaptive_'+loss+'_'+dataset_name+'.pdf', bbox_inches='tight')\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272b208f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sets = ['mushrooms', 'ijcnn', 'rcv1']\n",
    "losses = ['MSELoss', 'BCEWithLogitsLoss']\n",
    "wandb_records = pd.read_csv('logs/wandb_data/__full__'+SUMMARY_FILE, header=0, squeeze=True)\n",
    " \n",
    "for data_set in data_sets:\n",
    "    for loss in losses: \n",
    "        print('generating Adagrad plot for ', data_set, loss)\n",
    "        generate_A2_figure(loss, data_set, wandb_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bc88a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40095e4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febbd3df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
