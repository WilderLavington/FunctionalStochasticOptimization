{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6817e1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import wandb\n",
    "api = wandb.Api(timeout=180)\n",
    "import os\n",
    "import pandas as pd\n",
    "import wandb\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "from copy import deepcopy\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import argparse\n",
    "import itertools\n",
    "import  matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import itertools\n",
    "import time\n",
    "import matplotlib as mpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88cc4b7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists\n"
     ]
    }
   ],
   "source": [
    "USER='wilderlavington'\n",
    "PROJECT='FunctionalStochasticOptimization'\n",
    "SUMMARY_FILE='sharan_report_0906.csv'\n",
    "K=1\n",
    "# make plots dir\n",
    "try:\n",
    "    os.makedirs(\"plots/0906/\")\n",
    "except FileExistsError:\n",
    "    print(\"File already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afb85a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_wandb_summary(sweeps=['ijsmzrdx', 'l9x8gji5', 'hk7t69nt', 'ircrqll7']):\n",
    "    \"\"\"\n",
    "    Download a summary of all runs on the wandb project\n",
    "    \"\"\"\n",
    "    runs = api.runs(USER+'/'+PROJECT, per_page=10000000)\n",
    "    summary_list = []\n",
    "    config_list = []\n",
    "    name_list = []\n",
    "    id_list = []\n",
    "    assert len([run for run in runs])\n",
    "    for run in tqdm(runs):\n",
    "        if (sweeps is not None):\n",
    "            if (run.sweep is not None) and (run.sweep.id in sweeps):\n",
    "                summary_list.append(run.summary._json_dict)\n",
    "                run = api.run(USER+'/'+PROJECT+\"/\"+run.id)\n",
    "                config_list.append({k: v for k, v in run.config.items()})\n",
    "                name_list.append(run.name)\n",
    "                id_list.append(run.id)\n",
    "        elif sweeps is None:\n",
    "            summary_list.append(run.summary._json_dict)\n",
    "            run = api.run(USER+'/'+PROJECT+\"/\"+run.id)\n",
    "            config_list.append({k: v for k, v in run.config.items()})\n",
    "            name_list.append(run.name)\n",
    "            id_list.append(run.id)\n",
    "        else:\n",
    "            pass\n",
    "    summary_df = pd.DataFrame.from_records(summary_list)\n",
    "    config_df = pd.DataFrame.from_records(config_list)\n",
    "    name_df = pd.DataFrame({\"name\": name_list, \"id\": id_list})\n",
    "    all_df = pd.concat([name_df, config_df, summary_df], axis=1)\n",
    "    Path('logs/wandb_data/').mkdir(parents=True, exist_ok=True)\n",
    "    all_df.to_csv('logs/wandb_data/'+SUMMARY_FILE)\n",
    "\n",
    "def download_wandb_records():\n",
    "    \"\"\"\n",
    "    Download data for all runs in summary file\n",
    "    \"\"\"\n",
    "    # load it all in and clean it up\n",
    "    runs_df = pd.read_csv('logs/wandb_data/'+SUMMARY_FILE, header=0, squeeze=True)\n",
    "    runs_df = runs_df.loc[:,~runs_df.columns.duplicated()]\n",
    "    columns_of_interest = ['avg_loss', 'optim_steps', 'grad_norm', 'time_elapsed', \\\n",
    "             'grad_evals', 'function_evals']\n",
    "    # set which columns we will store for vizualization\n",
    "    list_of_dataframes = []\n",
    "    # iterate through all runs to create individual databases\n",
    "    for ex in tqdm(range(len(runs_df)), leave=False):\n",
    "        # get the associated runs\n",
    "        run = api.run(USER+'/'+PROJECT+'/'+runs_df.loc[runs_df.iloc[ex,0],:]['id'])\n",
    "        run_df = []\n",
    "        # iterate through all rows in online database\n",
    "        base_info = {}\n",
    "        for key in runs_df.loc[runs_df.iloc[ex,0],:].keys():\n",
    "            base_info.update({key:runs_df.loc[runs_df.iloc[ex,0],:][key]})\n",
    "        for i, row in run.history().iterrows():\n",
    "            row_info = deepcopy(base_info)\n",
    "            row_info.update({key:row[key] for key in columns_of_interest if key in row.keys()})\n",
    "            run_df.append(row_info)\n",
    "        # convert format to dataframe and add to our list\n",
    "        list_of_dataframes.append(pd.DataFrame(run_df))\n",
    "    # combine and then store\n",
    "    wandb_records = pd.concat(list_of_dataframes)\n",
    "    wandb_records.to_csv('logs/wandb_data/__full__'+SUMMARY_FILE)\n",
    "    # return single data frame for vizualization\n",
    "    return wandb_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fa6653",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|█▎                                   | 714/20922 [06:27<2:12:00,  2.55it/s]"
     ]
    }
   ],
   "source": [
    "download_wandb_summary()\n",
    "wandb_records = download_wandb_records()\n",
    "wandb_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7f7a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth(array, k):\n",
    "    array = np.array(array)\n",
    "    new_array = deepcopy(array)\n",
    "    # print(array[max(0,i-k):i] )\n",
    "    for i in range(len(array)):\n",
    "        if str(array[i]) != 'nan':\n",
    "            avg_list = [val for val in array[max(0,i-k):i+1] if str(val) != 'nan']\n",
    "            new_array[i] = sum(avg_list) / len(avg_list)\n",
    "    return new_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ea348d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_dataframe(records, id_subfields={}, avg_subfields=['seed'],\n",
    "            max_subfields=['log_eta', 'eta_schedule', 'c'],\n",
    "            x_col='optim_steps', y_col='avg_loss'):\n",
    "    #\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    max_subfields = [m for m in max_subfields if m not in id_subfields.keys()]\n",
    "\n",
    "    for key in id_subfields: \n",
    "        records = records.loc[records[key] == id_subfields[key]] \n",
    "    records['function_evals+grad_evals'] = records['function_evals']+records['grad_evals']\n",
    "    if not len(records):\n",
    "        return None\n",
    "    # remove nans\n",
    "    records = records[records[y_col].notna()]\n",
    "    important_cols = list(set(avg_subfields+max_subfields+\\\n",
    "        list(id_subfields.keys())+[x_col, y_col, 'optim_steps']))\n",
    "    # remove redundant information\n",
    "    records = records[important_cols]\n",
    "    # average over avg_subfields\n",
    "    records = records.drop(avg_subfields, axis=1)\n",
    "    # group over averaging field\n",
    "    gb = list(set(list(max_subfields+list(id_subfields.keys())+[x_col, 'optim_steps'])))\n",
    "    # only look at final optim steps\n",
    "    last_mean_records = records.loc[records['optim_steps'] == records['optim_steps'].max()]\n",
    "    # get the best record\n",
    "    best_record = last_mean_records[last_mean_records[y_col] == last_mean_records[y_col].min()]\n",
    "    # find parameters of the best record\n",
    "    merge_on = list(set(gb)-set(['optim_steps', x_col, y_col]))\n",
    "    merge_on = [ x for x in merge_on if x in best_record.columns.values]\n",
    "    best_records = pd.merge(best_record[merge_on], records, on=merge_on,how='left')\n",
    "    final_records = best_records.groupby(merge_on+[x_col], as_index=False)[y_col].mean()\n",
    "    final_records[y_col+'25'] = best_records.groupby(merge_on+[x_col], as_index=False)[y_col].quantile(0.25)[y_col]\n",
    "    final_records[y_col+'75'] = best_records.groupby(merge_on+[x_col], as_index=False)[y_col].quantile(0.75)[y_col]\n",
    "    final_records = final_records.sort_values(x_col, axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last')\n",
    "    # smooth outputs \n",
    "    final_records[y_col+'75'] = smooth(final_records[y_col+'75'],K)\n",
    "    final_records[y_col+'25'] = smooth(final_records[y_col+'25'],K)\n",
    "    final_records[y_col] = smooth(final_records[y_col],K) \n",
    "#     print(best_record['algo'].unique())\n",
    "#     print(best_record)\n",
    "    return final_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b7836a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_plot(proc_df, x, y, ax, label, linestyle='solid', color=None, x_max=100000):\n",
    "    low_order_idx = (torch.tensor(proc_df[x].values) < x_max).nonzero().reshape(-1)\n",
    "    if label:\n",
    "        ax.plot(torch.tensor(proc_df[x].values[low_order_idx]), \n",
    "                torch.tensor(proc_df[y].values[low_order_idx]), \n",
    "                label=label, linestyle=linestyle, color=color,\n",
    "                linewidth=4)\n",
    "    else:\n",
    "        ax.plot(torch.tensor(proc_df[x].values[low_order_idx]), \n",
    "                torch.tensor(proc_df[y].values[low_order_idx]), \n",
    "                label='_nolegend_', linestyle=linestyle, color=color,\n",
    "                linewidth=4)\n",
    "    ax.fill_between(torch.tensor(proc_df[x].values)[low_order_idx],\n",
    "            torch.tensor(proc_df[y+'75'].values)[low_order_idx],\n",
    "            torch.tensor(proc_df[y+'25'].values)[low_order_idx],\n",
    "            alpha = 0.5, label='_nolegend_', linestyle=linestyle, color=color)\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9f1a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_A1_figure(loss, dataset_name, wandb_records):\n",
    "    \n",
    "    # base info   \n",
    "    schedules = ['constant', 'stochastic', 'exponential']\n",
    "    batch_sizes = [5, 25, 125, 625]\n",
    "    m = [1, 2, 5, 10, 20]\n",
    "    x = 'grad_evals'\n",
    "    y = 'grad_norm'\n",
    "    \n",
    "    # init plots \n",
    "    fig, axs = plt.subplots(len(schedules), len(batch_sizes), figsize=(21, 21), sharey=True)\n",
    "    colors = mpl.cm.Set1.colors   # Qualitative colormap\n",
    "    colormap = {'SGD':colors[0], 'SLS':colors[1]}\n",
    "    colormap.update({'FuncOpt'+str(m_):colors[idx+2] for idx, m_ in enumerate(m)})\n",
    "    algorithms = ['SGD', 'SLS'] + ['FuncOpt'+str(m_) for m_ in m]\n",
    "    plt.title('Comparison of SGD/SLS/FuncOpt: '+loss+'-'+dataset_name)\n",
    "    label_map = {x:'Time-Elapsed', y:'Gradient-Norm'}\n",
    "    # now add in the lines to each of the plots \n",
    "    for row, schedule in enumerate(schedules):\n",
    "        for col, batch_size in enumerate(batch_sizes):\n",
    "            x_max = 0\n",
    "            # SLS\n",
    "            proc_df = format_dataframe(wandb_records,\n",
    "                id_subfields={'batch_size': batch_size, 'episodes': EPISODES,\n",
    "                'use_optimal_stepsize': 1, 'loss': loss, 'algo': 'LSOpt',\n",
    "                'eta_schedule': schedule, 'dataset_name': dataset_name},\n",
    "                x_col=x , y_col=y)\n",
    "            if proc_df is not None:\n",
    "                x_max = max(proc_df[x].values.max(), x_max)\n",
    "                axs[row][col] = generate_plot(proc_df, x, y, axs[row][col], label='SLS', \n",
    "                                             linestyle='solid', color=colormap['SLS'])\n",
    "            else:\n",
    "                print('missing SLS  ', schedule, batch_size)\n",
    "            \n",
    "            # SGD\n",
    "            proc_df = format_dataframe(wandb_records,\n",
    "                id_subfields={'batch_size': batch_size, 'episodes': EPISODES,\n",
    "                'use_optimal_stepsize': 1, 'loss': loss, 'algo': 'SGD',\n",
    "                'eta_schedule': schedule, 'dataset_name': dataset_name},\n",
    "                x_col=x , y_col=y)\n",
    "            if proc_df is not None: \n",
    "                x_max = max(proc_df[x].values.max(), x_max)\n",
    "                axs[row][col] = generate_plot(proc_df, x, y, axs[row][col], label='SGD', \n",
    "                                             linestyle='solid', color=colormap['SGD'])\n",
    "            else:\n",
    "                print('missing SGD  ', schedule, batch_size)\n",
    "            \n",
    "            # FMDopt theoretical \n",
    "            for m_ in m:\n",
    "                # create parsed info \n",
    "                proc_df = format_dataframe(wandb_records, \n",
    "                    id_subfields={'batch_size': batch_size, 'episodes': EPISODES,\n",
    "                        'use_optimal_stepsize': 1,  \n",
    "                        'loss': loss, 'algo': 'SGD_FMDOpt', 'm': m_,\n",
    "                        'eta_schedule': schedule, 'dataset_name': dataset_name}, \n",
    "                         avg_subfields=['seed'], max_subfields=['c'],\n",
    "                    x_col=x, y_col=y)\n",
    "                if proc_df is not None:\n",
    "                    x_max = max(proc_df[x].values.max(), x_max)\n",
    "                    axs[row][col] = generate_plot(proc_df, x, y, axs[row][col],  \\\n",
    "                                            label='FuncOpt'+str(m_), linestyle='solid', color=colormap['FuncOpt'+str(m_)])\n",
    "                else:\n",
    "                    print('missing FMDopt  ', m_, schedule, batch_size)\n",
    "            \n",
    "            axs[row][col].set_xlim(0, x_max)  \n",
    "            axs[row][col].grid()    \n",
    "            axs[row][col].set_title('schedule: '+schedule+', batch_size: '+str(batch_size), fontsize=18)\n",
    "            axs[row][col].set_xlabel(label_map[x], fontsize=16)\n",
    "            axs[row][col].set_ylabel(label_map[y], fontsize=16)\n",
    "            axs[row][col].tick_params(axis='both', which='major', labelsize=14)\n",
    "            axs[row][col].tick_params(axis='both', which='minor', labelsize=14)\n",
    "    \n",
    "    # remaining format stuff  \n",
    "    handles = [mpatches.Patch(color=colormap[algo], label=algo) for algo in algorithms]\n",
    "    leg = fig.legend(handles=handles,\n",
    "           loc=\"lower center\",   # Position of legend\n",
    "           borderaxespad=1.65,    # Small spacing around legend box\n",
    "           # title=\"Algorithms\",  # Title for the legend\n",
    "           fontsize=18,\n",
    "           ncol=7, \n",
    "           bbox_to_anchor=(0.5, -0.05),\n",
    "           )\n",
    "    plt.yscale(\"log\")\n",
    "    plt.subplots_adjust(hspace=1.5)\n",
    "    plt.rcParams['figure.dpi'] = 400 \n",
    "    plt.suptitle('Comparison of SGD/SLS/FuncOpt: Loss: '+loss+', Dataset: '+dataset_name, fontsize=28)\n",
    "    fig.tight_layout()\n",
    "    fig.subplots_adjust(top=0.95)\n",
    "    \n",
    "    # show / save\n",
    "    plt.savefig('plots/0902/'+loss+'_'+dataset_name+'.pdf', bbox_inches='tight')\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4555686",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sets = ['mushrooms', 'ijcnn', 'rcv1']\n",
    "losses = ['MSELoss', 'BCEWithLogitsLoss']\n",
    "wandb_records = pd.read_csv('logs/wandb_data/__full__'+SUMMARY_FILE, header=0, squeeze=True)\n",
    " \n",
    "for data_set in data_sets:\n",
    "    for loss in losses:\n",
    "        print('generating SGD plot for ', data_set, loss)\n",
    "        generate_A1_figure(loss, data_set, wandb_records) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaeddb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_A2_figure(loss, dataset_name, wandb_records):\n",
    "    \n",
    "    # base info   \n",
    "    schedules = ['constant']\n",
    "    batch_sizes = [5, 25, 125, 625]\n",
    "    m = [1, 2, 5, 10, 20]\n",
    "    x = 'grad_evals'\n",
    "    y = 'grad_norm'\n",
    "    \n",
    "    # init plots \n",
    "    fig, axs = plt.subplots(len(schedules), len(batch_sizes), figsize=(21, 7), sharey=True)\n",
    "    colors = mpl.cm.Set1.colors   # Qualitative colormap\n",
    "    colormap = {'Adagrad':colors[0]}\n",
    "    colormap.update({'AdaFuncOpt'+str(m_):colors[idx+2] for idx, m_ in enumerate(m)})\n",
    "    axs = [axs]\n",
    "    label_map = {x:'Time-Elapsed', y:'Gradient-Norm'}\n",
    "    algorithms = ['Adagrad'] + ['AdaFuncOpt'+str(m_) for m_ in m]\n",
    "    \n",
    "    # now add in the lines to each of the plots \n",
    "    for row, schedule in enumerate(schedules):\n",
    "        for col, batch_size in enumerate(batch_sizes):\n",
    "            x_max = 0\n",
    "            \n",
    "            # adagrad\n",
    "            proc_df = format_dataframe(wandb_records,\n",
    "                id_subfields={'batch_size': batch_size, 'episodes': EPISODES,\n",
    "                'use_optimal_stepsize': 1, 'loss': loss, 'algo': 'Sadagrad',\n",
    "                'eta_schedule': schedule, 'dataset_name': dataset_name},\n",
    "                x_col=x , y_col=y)\n",
    "            if proc_df is not None: \n",
    "                x_max = max(proc_df[x].values.max(), x_max)\n",
    "                axs[row][col] = generate_plot(proc_df, x, y, axs[row][col], label='Sadagrad', \n",
    "                                             linestyle='solid', color=colormap['Adagrad'])\n",
    "            else:\n",
    "                print('missing sadagrad  ', schedule, batch_size)\n",
    "    \n",
    "            # AdaFMDopt theoretical \n",
    "            for m_ in m:\n",
    "                # create parsed info \n",
    "                proc_df = format_dataframe(wandb_records, \n",
    "                    id_subfields={'batch_size': batch_size, 'episodes': EPISODES,\n",
    "                        'use_optimal_stepsize': 0, 'log_eta': -3,\n",
    "                        'loss': loss, 'algo': 'Ada_FMDOpt', 'm': m_,\n",
    "                        'eta_schedule': schedule, 'dataset_name': dataset_name}, \n",
    "                         avg_subfields=['seed'], max_subfields=['c'],\n",
    "                    x_col=x, y_col=y)\n",
    "                \n",
    "                # generate the associated plot \n",
    "                if proc_df is not None:\n",
    "                    axs[row][col] = generate_plot(proc_df, x, y, axs[row][col],  \\\n",
    "                                            label='AdaFuncOpt', linestyle='solid', color=colormap['AdaFuncOpt'+str(m_)])\n",
    "                else:\n",
    "                    print('missing AdaFMDopt  ', schedule, batch_size)\n",
    "            \n",
    "            axs[row][col].set_xlim(0, x_max)  \n",
    "            axs[row][col].grid()    \n",
    "            axs[row][col].set_title('schedule: '+schedule+', batch_size: '+str(batch_size), fontsize=20)\n",
    "            axs[row][col].set_xlabel(label_map[x], fontsize=16)\n",
    "            axs[row][col].set_ylabel(label_map[y], fontsize=16)\n",
    "            axs[row][col].tick_params(axis='both', which='major', labelsize=14)\n",
    "            axs[row][col].tick_params(axis='both', which='minor', labelsize=14)\n",
    "    \n",
    "    # remaining format stuff \n",
    "    handles = [mpatches.Patch(color=colormap[algo], label=algo) for algo in algorithms]\n",
    "    leg = fig.legend(handles=handles,\n",
    "           loc=\"lower center\",   # Position of legend\n",
    "           borderaxespad=1.65,    # Small spacing around legend box\n",
    "           # title=\"Algorithms\",  # Title for the legend\n",
    "           fontsize=18,\n",
    "           ncol=6, \n",
    "           bbox_to_anchor=(0.5, -0.15),\n",
    "           ) \n",
    "    plt.yscale(\"log\")\n",
    "    plt.subplots_adjust(hspace=1.25)\n",
    "    plt.rcParams['figure.dpi'] = 400 \n",
    "    plt.suptitle('Comparison of Adagrad/Ada-FuncOpt: Loss: '+loss+', Dataset: '+dataset_name, fontsize=28)\n",
    "    fig.tight_layout()\n",
    "    fig.subplots_adjust(top=0.85)\n",
    "    \n",
    "    # show / save\n",
    "    plt.savefig('plots/0902/Adaptive_'+loss+'_'+dataset_name+'.pdf', bbox_inches='tight')\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272b208f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sets = ['mushrooms', 'ijcnn', 'rcv1']\n",
    "losses = ['MSELoss', 'BCEWithLogitsLoss']\n",
    "wandb_records = pd.read_csv('logs/wandb_data/__full__'+SUMMARY_FILE, header=0, squeeze=True)\n",
    " \n",
    "for data_set in data_sets:\n",
    "    for loss in losses: \n",
    "        print('generating Adagrad plot for ', data_set, loss)\n",
    "        generate_A2_figure(loss, data_set, wandb_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bc88a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40095e4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febbd3df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
